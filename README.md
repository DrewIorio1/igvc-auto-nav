The following code was developed for the 2025 IGVC auto navigation competition. It commenced in March 2025, with the objective of completion by the end of May 2025. The code is implemented in Python as a fusion node to perform lane following, object detection, map finding utilizing RRT*, waypoint navigation, and velocity commands for two motors. It was written at the University of Bridgeport with support and assistance from Ali Hamadeen and Rudra Mitra. 

![image](https://github.com/user-attachments/assets/9a785ec3-88bb-4d29-98a0-2ad5f02fa4b9)

The vehicle's autonomous navigation system, based on the Robot Operating System (ROS2), is regulated by the Fusion Controller node, which oversees data integration from various sources, including localization, LiDAR, cameras, GPS, path planning, and control velocity. The localization system, as detailed in Section 6.2, acquires the x and y coordinates, as well as the angle at which the vehicle is heading, through information obtained from the motors and the Inertial Measurement Unit (IMU). Subsequently, the process node determines, based on established thresholds, whether to utilize the camera or LiDAR system. The camera is employed in scenarios where lanes are present, and only a single obstacle is detected; conversely, LiDAR is utilized when an obstacle is identified within a range of four to five meters. Based on the camera input and the established thresholds, the system assesses the availability of lanes and ascertains whether waypoints or obstacles necessitate navigation. These inputs identify the goal points, after which the start and goal points are provided to the RRT* algorithm for the establishment of the optimal path. Ultimately, the speed commands are generated by the control velocity controller, which determines the appropriate speed—ranging from one to five miles per hour—to dispatch to each motor. Each motor operates independently; therefore, the control velocity must account for the differential slip of each tire for every command issued.  

